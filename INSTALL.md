# ì„¤ì¹˜ ê°€ì´ë“œ

ë³¸ í”„ë¡œì íŠ¸ëŠ” **Mac(Ollama)** ê³¼ **GPU(vLLM)** í™˜ê²½ì„ ëª¨ë‘ ì§€ì›í•˜ë©°, ìžë™ìœ¼ë¡œ í™˜ê²½ì„ ê°ì§€í•©ë‹ˆë‹¤.

## ë©€í‹°í„´ ìµœì í™”

- **Mac (Ollama)**: KV Cache ìžë™ ê´€ë¦¬ë¡œ ëŒ€í™” ë§¥ë½ ìœ ì§€
- **GPU (vLLM)**: Prefix Cachingìœ¼ë¡œ 2~3ë°° ë¹ ë¥¸ ë©€í‹°í„´ ì‘ë‹µ

---

## 1ï¸âƒ£ Mac í™˜ê²½ (Ollama)

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- macOS (Apple Silicon ë˜ëŠ” Intel)
- Python 3.11+
- Ollama ì„¤ì¹˜ í•„ìš”

### ì„¤ì¹˜ ë‹¨ê³„

**1. Ollama ì„¤ì¹˜**
```bash
# Homebrewë¡œ ì„¤ì¹˜
brew install ollama

# ë˜ëŠ” ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ
# https://ollama.ai/download
```

**2. Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**
```bash
# Llama 3.1 8B ëª¨ë¸ ì„¤ì¹˜ (4ë¹„íŠ¸ ì–‘ìží™”)
ollama pull llama3.1:8b-instruct-q4_K_M
```

**3. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
pip install -r requirements-mac.txt
```

**4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
`.env` íŒŒì¼ì—ì„œ Ollama ëª¨ë¸ëª… í™•ì¸:
```bash
OLLAMA_MODEL_NAME="llama3.1:8b-instruct-q4_K_M"
```

**5. ì‹¤í–‰**
```bash
python main.py
```

### íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš© (ì„ íƒì‚¬í•­)

Ollamaì—ì„œ íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´:

**1. Modelfile ìƒì„±**
```bash
cat > Modelfile << EOF
FROM ./Finetuning_Models/llama_3.1
TEMPLATE """{{ .System }}

{{ .Prompt }}"""
PARAMETER temperature 0.3
PARAMETER top_p 0.9
EOF
```

**2. ëª¨ë¸ ìƒì„±**
```bash
ollama create counselor-finetuned -f Modelfile
```

**3. `.env` íŒŒì¼ ìˆ˜ì •**
```bash
OLLAMA_MODEL_NAME="counselor-finetuned"
```

---

## 2ï¸âƒ£ GPU í™˜ê²½ (vLLM)

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- Linux ë˜ëŠ” Windows (WSL2)
- NVIDIA GPU (CUDA 12.0+)
- Python 3.11+
- ìµœì†Œ 16GB GPU ë©”ëª¨ë¦¬ (Llama 3.1 8B ê¸°ì¤€)

### ì„¤ì¹˜ ë‹¨ê³„

**1. CUDA ì„¤ì¹˜ í™•ì¸**
```bash
nvidia-smi
```

**2. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
pip install -r requirements-gpu.txt
```

**3. HuggingFace ë¡œê·¸ì¸ (Llama ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”)**
```bash
huggingface-cli login
```

**4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
`.env` íŒŒì¼ì—ì„œ LoRA ì–´ëŒ‘í„° ê²½ë¡œ í™•ì¸:
```bash
FINETUNED_MODEL_PATH="./Finetuning_Models/llama_3.1"
```

**5. ì‹¤í–‰**
```bash
python main.py
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ

`models/vllm_wrapper.py:48` ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¡°ì •:
```python
gpu_memory_utilization=0.7  # ê¸°ë³¸ê°’ 0.9ì—ì„œ ë‚®ì¶¤
```

---

## 3ï¸âƒ£ í™˜ê²½ ê°•ì œ ì§€ì • (ì„ íƒì‚¬í•­)

ìžë™ ê°ì§€ë¥¼ ë¬´ì‹œí•˜ê³  íŠ¹ì • í™˜ê²½ì„ ê°•ì œí•˜ë ¤ë©´:

`.env` íŒŒì¼ì— ì¶”ê°€:
```bash
FORCE_ENVIRONMENT="mac"   # ë˜ëŠ” "gpu"
```

---

## 4ï¸âƒ£ í™•ì¸ ì‚¬í•­

### Mac í™˜ê²½
```bash
# Ollama ì‹¤í–‰ í™•ì¸
ollama list

# ëª¨ë¸ì´ ìžˆëŠ”ì§€ í™•ì¸
ollama list | grep llama3.1
```

### GPU í™˜ê²½
```bash
# CUDA í™•ì¸
nvidia-smi

# PyTorch CUDA í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
```

---

## ðŸ“ íŒ¨í‚¤ì§€ êµ¬ì¡°

```
requirements-base.txt     # ê³µí†µ íŒ¨í‚¤ì§€
requirements-mac.txt      # Macìš© (Ollama + Transformers)
requirements-gpu.txt      # GPUìš© (vLLM)
requirements.txt          # ê¸°ì¡´ íŒŒì¼ (ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨)
```

---

## ðŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Ollama ì—°ê²° ì˜¤ë¥˜
```bash
# Ollama ì„œë²„ ìž¬ì‹œìž‘
brew services restart ollama
```

### vLLM ì„¤ì¹˜ ì˜¤ë¥˜
```bash
# CUDA ë²„ì „ í™•ì¸ í›„ í˜¸í™˜ë˜ëŠ” PyTorch ì„¤ì¹˜
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

### ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜
```bash
# í™˜ê²½ ê°ì§€ í™•ì¸
python -c "from models.env_detector import detect_environment; print(detect_environment())"
```

---

## ðŸš€ ì„±ëŠ¥ ë¹„êµ

| í™˜ê²½ | ì´ˆê¸° ì‘ë‹µ | ë©€í‹°í„´ ì‘ë‹µ | ë©”ëª¨ë¦¬ |
|------|-----------|-------------|--------|
| **Mac (Ollama)** | ~2ì´ˆ | ~1ì´ˆ (KV cache) | ~4GB |
| **GPU (vLLM)** | ~1ì´ˆ | ~0.3ì´ˆ (Prefix cache) | ~14GB |

---

## ë¬¸ì˜

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”!
