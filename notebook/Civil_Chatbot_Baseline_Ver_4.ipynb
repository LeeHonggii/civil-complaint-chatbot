{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Civil_Chatbot_Baseline_Ver_4: SFT + QLoRA\n",
        "##멋사 상담랜드"
      ],
      "metadata": {
        "id": "ujtxyIHLMUIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ysTLQEEjMRyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements"
      ],
      "metadata": {
        "id": "c6hNXArLMcU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q transformers datasets accelerate peft bitsandbytes trl\n",
        "\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")"
      ],
      "metadata": {
        "id": "e0wZfEKAMYJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wandb login"
      ],
      "metadata": {
        "id": "YKr5K9pfMzPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import wandb\n",
        "except ImportError:\n",
        "    wandb = None  # type: ignore\n",
        "from google.colab import userdata\n",
        "\n",
        "if wandb is None:\n",
        "    raise ImportError(\"wandb is not installed. Please install it using the setup cell above.\")\n",
        "##Use Wandb Api-key\n",
        "wandb.login(key=userdata.get('WANDBKEY'))\n",
        "print(\"Weights & Biases login complete\")"
      ],
      "metadata": {
        "id": "9JS1qGvZM1LK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "QHP0NsGaMrMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    GenerationConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "try:\n",
        "    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
        "except ImportError:\n",
        "    LoraConfig = None  # type: ignore\n",
        "    TaskType = None  # type: ignore\n",
        "\n",
        "    def get_peft_model(*args, **kwargs):\n",
        "        raise ImportError(\"peft 패키지가 필요합니다. 위 설치 셀을 실행해 주세요.\")\n",
        "\n",
        "    def prepare_model_for_kbit_training(*args, **kwargs):\n",
        "        raise ImportError(\"peft 패키지가 필요합니다. 위 설치 셀을 실행해 주세요.\")\n",
        "\n",
        "try:\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "except ImportError:\n",
        "    SFTTrainer = None  # type: ignore\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"baseline4\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "##드라이브 경로 지정\n",
        "PROJECT_ROOT = Path(r\"/content/drive/MyDrive/Dev/Civil_Chatbot\")\n",
        "DATA_ROOT = PROJECT_ROOT\n",
        "\n",
        "TRAIN_JSONL = Path(r\"/content/drive/MyDrive/Dev/Civil_Chatbot/raw_data_messages.jsonl\")\n",
        "VAL_JSONL = Path(r\"/content/drive/MyDrive/Dev/Civil_Chatbot/raw_val_data_messages.jsonl\")\n",
        "\n",
        "if not TRAIN_JSONL.exists():\n",
        "    TRAIN_JSONL = DATA_ROOT / \"raw_data_messages.jsonl\"\n",
        "if not VAL_JSONL.exists():\n",
        "    VAL_JSONL = DATA_ROOT / \"raw_val_data_messages.jsonl\"\n",
        "\n",
        "BASE_MODEL_NAME = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"results\" / \"baseline4\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EVAL_LOG_DIR = PROJECT_ROOT / \"outputs\" / \"baseline4_eval\"\n",
        "EVAL_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "WANDB_PROJECT = os.environ.get(\"WANDB_PROJECT\", \"baseline4-korean-chatbot\")\n",
        "WANDB_ENTITY = os.environ.get(\"WANDB_ENTITY\")\n",
        "WANDB_RUN_NAME_PREFIX = os.environ.get(\"WANDB_RUN_NAME_PREFIX\", \"baseline4\")\n",
        "\n",
        "print(f\"Train path: {TRAIN_JSONL}\")\n",
        "print(f\"Val path:   {VAL_JSONL}\")\n",
        "print(f\"Result dir: {OUTPUT_DIR}\")\n",
        "print(f\"Eval dir:   {EVAL_LOG_DIR}\")\n",
        "print(f\"W&B project: {WANDB_PROJECT}\")\n"
      ],
      "metadata": {
        "id": "HQpejzLvMqLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Normalization"
      ],
      "metadata": {
        "id": "MLTrFyGeNE-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROLE_KEY_MAP = {\n",
        "    \"user\": \"user\",\n",
        "    \"assistant\": \"assistant\",\n",
        "    \"system\": \"system\",\n",
        "    \"assistance\": \"assistant\",\n",
        "    \"assistant_response\": \"assistant\",\n",
        "}\n",
        "\n",
        "\n",
        "def sanitize_text(text: Optional[str]) -> str:\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    value = str(text).replace(\"\\r\\n\", \"\\n\").replace(\"\\u3000\", \" \" ).strip()\n",
        "    return value\n",
        "\n",
        "\n",
        "def normalize_turn(turn: Dict[str, str]) -> Optional[Dict[str, str]]:\n",
        "    for key, value in turn.items():\n",
        "        role = ROLE_KEY_MAP.get(key)\n",
        "        if role and value is not None:\n",
        "            cleaned = sanitize_text(value)\n",
        "            if cleaned:\n",
        "                return {\"role\": role, \"content\": cleaned}\n",
        "    return None\n",
        "\n",
        "\n",
        "def normalize_messages(messages: Iterable[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "    normalized: List[Dict[str, str]] = []\n",
        "    for turn in messages:\n",
        "        normalized_turn = normalize_turn(turn)\n",
        "        if normalized_turn:\n",
        "            normalized.append(normalized_turn)\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def is_valid_dialogue(messages: List[Dict[str, str]], min_pairs: int = 1) -> bool:\n",
        "    if not messages:\n",
        "        return False\n",
        "    user_count = sum(1 for m in messages if m[\"role\"] == \"user\")\n",
        "    assistant_count = sum(1 for m in messages if m[\"role\"] == \"assistant\")\n",
        "    return user_count >= min_pairs and assistant_count >= min_pairs\n",
        "\n",
        "\n",
        "def count_conversation_pairs(messages: List[Dict[str, str]]) -> int:\n",
        "    user_turns = 0\n",
        "    assistant_turns = 0\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            user_turns += 1\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            assistant_turns += 1\n",
        "    return min(user_turns, assistant_turns)\n",
        "\n",
        "\n",
        "def load_message_datasets(train_path: Path, val_path: Path) -> DatasetDict:\n",
        "    if not train_path.exists():\n",
        "        raise FileNotFoundError(f\"Train data not found: {train_path}\")\n",
        "    if not val_path.exists():\n",
        "        raise FileNotFoundError(f\"Validation data not found: {val_path}\")\n",
        "    data_files = {\"train\": str(train_path), \"validation\": str(val_path)}\n",
        "    dataset_dict = load_dataset(\"json\", data_files=data_files)\n",
        "    logger.info(\"Loaded dataset: %s\", dataset_dict)\n",
        "    return dataset_dict\n",
        "\n",
        "\n",
        "def add_normalized_messages(dataset: DatasetDict) -> DatasetDict:\n",
        "    def _normalize(example):\n",
        "        return {\"normalized_messages\": normalize_messages(example.get(\"messages\", []))}\n",
        "\n",
        "    return dataset.map(_normalize, desc=\"normalize messages\")\n",
        "\n",
        "\n",
        "def filter_dialogues(dataset: DatasetDict, min_pairs: int = 1) -> DatasetDict:\n",
        "    def _predicate(example):\n",
        "        return is_valid_dialogue(example[\"normalized_messages\"], min_pairs=min_pairs)\n",
        "\n",
        "    return dataset.filter(_predicate, desc=f\"filter dialogues >= {min_pairs} turns\")\n",
        "\n",
        "\n",
        "def summarize_dialogue_stats(dataset: DatasetDict) -> Dict[str, Dict[str, float]]:\n",
        "    stats: Dict[str, Dict[str, float]] = {}\n",
        "    for split_name, split in dataset.items():\n",
        "        lengths = [len(dialogue) for dialogue in split[\"normalized_messages\"]]\n",
        "        pair_counts = [count_conversation_pairs(dialogue) for dialogue in split[\"normalized_messages\"]]\n",
        "        if not lengths:\n",
        "            continue\n",
        "        stats[split_name] = {\n",
        "            \"num_samples\": len(lengths),\n",
        "            \"avg_turns\": float(np.mean(lengths)),\n",
        "            \"median_turns\": float(np.median(lengths)),\n",
        "            \"avg_pairs\": float(np.mean(pair_counts)),\n",
        "            \"max_turns\": int(np.max(lengths)),\n",
        "            \"min_turns\": int(np.min(lengths)),\n",
        "        }\n",
        "    return stats"
      ],
      "metadata": {
        "id": "XKwgK41CNG1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_message_datasets(TRAIN_JSONL, VAL_JSONL)\n",
        "processed_datasets = add_normalized_messages(raw_datasets)\n",
        "processed_datasets = filter_dialogues(processed_datasets, min_pairs=1)\n",
        "\n",
        "stats = summarize_dialogue_stats(processed_datasets)\n",
        "print(\"데이터 분포 요약:\")\n",
        "for split_name, values in stats.items():\n",
        "    formatted = \", \".join(f\"{key}={value}\" for key, value in values.items())\n",
        "    print(f\"- {split_name}: {formatted}\")\n",
        "\n",
        "sample_example = processed_datasets[\"train\"][0]\n",
        "print(\"샘플 대화 (앞부분):\")\n",
        "for turn in sample_example[\"normalized_messages\"][:4]:\n",
        "    print(turn)"
      ],
      "metadata": {
        "id": "BbYAp_-vNI5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat Template Alignment"
      ],
      "metadata": {
        "id": "06fomY3ANLDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token or \"<|end_of_text|>\"\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"Tokenizer eos_token: {tokenizer.eos_token!r}\")\n",
        "print(f\"Tokenizer pad_token: {tokenizer.pad_token!r}\")\n",
        "print(\"Chat template 존재 여부:\", bool(getattr(tokenizer, \"chat_template\", None)))"
      ],
      "metadata": {
        "id": "2Mq8PnIENOIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHAT_TEMPLATE_REGISTRY = {\n",
        "    \"llama-3\": \"\"\"{% for message in messages %}\n",
        "{{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n' + message['content'] | trim + '<|eot_id|>' }}\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}\n",
        "{{ '<|start_header_id|>assistant<|end_header_id|>\\n' }}\n",
        "{% endif %}\"\"\",\n",
        "    \"chatml\": \"\"\"{{ bos_token }}{% for message in messages %}\n",
        "{% if message['role'] == 'system' %}{{ '[SYSTEM]' + message['content'] | trim + '\n",
        "' }}{% elif message['role'] == 'user' %}{{ '[USER]' + message['content'] | trim + '\n",
        "' }}{% elif message['role'] == 'assistant' %}{{ '[ASSISTANT]' + message['content'] | trim + '\n",
        "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '[ASSISTANT]' }}{% endif %}\"\"\",\n",
        "    \"mistral-instruct\": \"\"\"{{ bos_token }}{% for message in messages %}\n",
        "{% if message['role'] == 'system' %}{{ '[INST] <<SYS>>\\n' + message['content'] | trim + '\n",
        "<</SYS>>\\n' }}{% elif message['role'] == 'user' %}{{ '[INST] ' + message['content'] | trim + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] | trim + '\n",
        "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '[INST]' }}{% endif %}\"\"\",\n",
        "}\n",
        "\n",
        "\n",
        "def set_chat_template(\n",
        "    tokenizer: AutoTokenizer,\n",
        "    template_key: Optional[str] = None,\n",
        "    custom_template: Optional[str] = None,\n",
        "    save_directory: Optional[Path] = None,\n",
        ") -> None:\n",
        "    if custom_template is not None:\n",
        "        tokenizer.chat_template = custom_template\n",
        "        if save_directory is not None:\n",
        "            save_directory.mkdir(parents=True, exist_ok=True)\n",
        "            tokenizer.save_pretrained(save_directory)\n",
        "        logger.info(\"chat_template updated from custom_template\")\n",
        "        return\n",
        "    if template_key is None:\n",
        "        raise ValueError(\"template_key 또는 custom_template 중 하나는 반드시 지정해야 합니다.\")\n",
        "    template = CHAT_TEMPLATE_REGISTRY.get(template_key)\n",
        "    if template is None:\n",
        "        raise KeyError(f\"지원하지 않는 template_key: {template_key}\")\n",
        "    tokenizer.chat_template = template\n",
        "    if save_directory is not None:\n",
        "        save_directory.mkdir(parents=True, exist_ok=True)\n",
        "        tokenizer.save_pretrained(save_directory)\n",
        "    logger.info(\"chat_template set to '%s'\", template_key)\n",
        "\n",
        "\n",
        "current_template_preview = (tokenizer.chat_template or \"<None>\")[:400]\n",
        "print(\"기존 chat_template (앞부분):\", current_template_preview)\n",
        "\n",
        "# 필요 시 템플릿 정렬\n",
        "set_chat_template(tokenizer, template_key=\"llama-3\", save_directory=None)\n",
        "\n",
        "updated_template_preview = (tokenizer.chat_template or \"<None>\")[:400]\n",
        "print(\"적용 후 chat_template (앞부분):\", updated_template_preview)\n",
        "print(\"사용 가능한 템플릿 키:\", list(CHAT_TEMPLATE_REGISTRY.keys()))"
      ],
      "metadata": {
        "id": "jAi5Y_0NNP_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat Formatting, generate input text"
      ],
      "metadata": {
        "id": "3tvHfT1BNUZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_chat_template_column(\n",
        "    dataset: Dataset,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    column_name: str = \"text\",\n",
        "    add_generation_prompt: bool = False,\n",
        ") -> Dataset:\n",
        "    def _convert(batch):\n",
        "        formatted = []\n",
        "        for conversation in batch[\"normalized_messages\"]:\n",
        "            formatted.append(\n",
        "                tokenizer.apply_chat_template(\n",
        "                    conversation,\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=add_generation_prompt,\n",
        "                )\n",
        "            )\n",
        "        return {column_name: formatted}\n",
        "\n",
        "    return dataset.map(\n",
        "        _convert,\n",
        "        batched=True,\n",
        "        desc=f\"format conversations -> {column_name}\",\n",
        "    )"
      ],
      "metadata": {
        "id": "pS0ElrviNSHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sft = add_chat_template_column(processed_datasets[\"train\"], tokenizer, column_name=\"text\", add_generation_prompt=False)\n",
        "val_sft = add_chat_template_column(processed_datasets[\"validation\"], tokenizer, column_name=\"text\", add_generation_prompt=False)\n",
        "\n",
        "sft_datasets = DatasetDict({\"train\": train_sft, \"validation\": val_sft})\n",
        "print(sft_datasets)\n",
        "\n",
        "formatted_sample = sft_datasets[\"train\"][0][\"text\"]\n",
        "print(\"포맷된 샘플 텍스트 (앞부분):\")\n",
        "print(formatted_sample[:600])"
      ],
      "metadata": {
        "id": "lngciubfNa3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QLora Setting"
      ],
      "metadata": {
        "id": "4NxwAUy3Ncrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quantization_config(load_in_4bit: bool = True) -> BitsAndBytesConfig:\n",
        "    if not load_in_4bit:\n",
        "        return BitsAndBytesConfig(load_in_4bit=False)\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_base_model(\n",
        "    model_name: str,\n",
        "    quantization_config: Optional[BitsAndBytesConfig] = None,\n",
        "    for_training: bool = False,\n",
        "    attn_implementation: Optional[str] = \"sdpa\",\n",
        "    device_map: Optional[Dict[str, str]] = None,\n",
        "):\n",
        "    model_kwargs: Dict[str, object] = {\n",
        "        \"torch_dtype\": torch.bfloat16,\n",
        "    }\n",
        "    if quantization_config is not None:\n",
        "        model_kwargs[\"quantization_config\"] = quantization_config\n",
        "        model_kwargs[\"device_map\"] = device_map or \"auto\"\n",
        "    elif device_map is not None:\n",
        "        model_kwargs[\"device_map\"] = device_map\n",
        "    if attn_implementation:\n",
        "        model_kwargs[\"attn_implementation\"] = attn_implementation\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "    if for_training:\n",
        "        model.config.use_cache = False\n",
        "        if quantization_config is not None and prepare_model_for_kbit_training is not None:\n",
        "            model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False) # Changed to False\n",
        "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "            model.gradient_checkpointing_enable()\n",
        "    else:\n",
        "        model.config.use_cache = True\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_lora_config(\n",
        "    r: int = 64,\n",
        "    alpha: int = 16,\n",
        "    dropout: float = 0.05,\n",
        "    bias: str = \"none\",\n",
        "    task_type: str = \"CAUSAL_LM\",\n",
        "    target_modules: Optional[List[str]] = None,\n",
        "):\n",
        "    if LoraConfig is None:\n",
        "        raise ImportError(\"peft.LoraConfig? ??? ? ????. peft ???? ??? ???.\")\n",
        "    if target_modules is None:\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    return LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        bias=bias,\n",
        "        task_type=TaskType.CAUSAL_LM if TaskType is not None else \"CAUSAL_LM\",\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "\n",
        "\n",
        "def apply_lora(model: AutoModelForCausalLM, lora_config: \"LoraConfig\"):\n",
        "    if get_peft_model is None:\n",
        "        raise ImportError(\"peft.get_peft_model? ??? ? ????. peft ???? ??? ???.\")\n",
        "    peft_model = get_peft_model(model, lora_config)\n",
        "    peft_model.print_trainable_parameters()\n",
        "    return peft_model\n",
        "\n",
        "\n",
        "def create_sft_trainer(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    train_dataset: Dataset,\n",
        "    eval_dataset: Optional[Dataset],\n",
        "    output_dir: Path,\n",
        "    learning_rate: float = 2e-4,\n",
        "    warmup_steps: int = 100,\n",
        "    per_device_train_batch_size: int = 2,\n",
        "    gradient_accumulation_steps: int = 4,\n",
        "    num_train_epochs: float = 2.0,\n",
        "    max_length: int = 512, # max_length 512, 768, 1024..\n",
        "    logging_steps: int = 25,\n",
        "    save_steps: int = 500,\n",
        "    run_name: Optional[str] = None,\n",
        "    report_to: Optional[List[str]] = None,\n",
        "    wandb_project: Optional[str] = None,\n",
        "):\n",
        "    if SFTTrainer is None:\n",
        "        raise ImportError(\"trl.SFTTrainer? ??? ? ????. trl ???? ??? ???.\")\n",
        "    use_bf16 = bool(torch.cuda.is_available() and getattr(torch.cuda, \"is_bf16_supported\", lambda: False)())\n",
        "    use_fp16 = bool(torch.cuda.is_available() and not use_bf16)\n",
        "    if report_to is None:\n",
        "        report_to = [\"wandb\", \"tensorboard\"] if wandb is not None else [\"tensorboard\"]\n",
        "    if wandb is not None and \"wandb\" in report_to:\n",
        "        os.environ.setdefault(\"WANDB_PROJECT\", wandb_project or WANDB_PROJECT)\n",
        "        if WANDB_ENTITY:\n",
        "            os.environ.setdefault(\"WANDB_ENTITY\", WANDB_ENTITY)\n",
        "\n",
        "    sft_config = SFTConfig(\n",
        "        output_dir=str(output_dir),\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        warmup_steps=warmup_steps,\n",
        "        max_steps=-1,\n",
        "\n",
        "        logging_steps=logging_steps,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=save_steps,\n",
        "        save_steps=save_steps,\n",
        "        save_total_limit=2,\n",
        "\n",
        "        bf16=use_bf16,\n",
        "        fp16=use_fp16,\n",
        "        gradient_checkpointing=True,\n",
        "\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        weight_decay=0.05,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "\n",
        "        report_to=report_to,\n",
        "        run_name=run_name,\n",
        "        dataloader_pin_memory=False,\n",
        "\n",
        "        # For SFT / 데이터 전처리 관련\n",
        "        max_length=512,\n",
        "        dataset_text_field=\"text\",\n",
        "        packing=False,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        processing_class=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        args=sft_config,\n",
        "    )\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "2YasMCymNd-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SFTTrainer"
      ],
      "metadata": {
        "id": "YanstGFBN-tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Set the environment variable to potentially help with fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "quant_config = get_quantization_config(load_in_4bit=True)\n",
        "qlora_base_model = load_base_model(\n",
        "    BASE_MODEL_NAME,\n",
        "    quantization_config=quant_config,\n",
        "    for_training=True,\n",
        ")\n",
        "lora_config = create_lora_config(r=64, alpha=16, dropout=0.05)\n",
        "qlora_model = apply_lora(qlora_base_model, lora_config)\n",
        "wandb_run = None\n",
        "if wandb is not None:\n",
        "    wandb_config = {\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"warmup_steps\": 100,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"per_device_train_batch_size\": 2,\n",
        "        \"num_train_epochs\": 2.0,\n",
        "        \"max_length\": 512,\n",
        "    }\n",
        "    wandb_run = wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        name=f\"{WANDB_RUN_NAME_PREFIX}_qlora\",\n",
        "        config=wandb_config,\n",
        "        reinit=True,\n",
        "    )\n",
        "trainer = create_sft_trainer(\n",
        "    model=qlora_model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=sft_datasets[\"train\"],\n",
        "    eval_dataset=sft_datasets[\"validation\"],\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=100,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=2.0,\n",
        "    max_length=512,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    run_name=(wandb_run.name if wandb_run is not None else f\"{WANDB_RUN_NAME_PREFIX}_qlora\"),\n",
        ")\n",
        "trainer.model.generation_config = GenerationConfig.from_model_config(trainer.model.config)"
      ],
      "metadata": {
        "id": "T_WZzy0rOAI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train Loop"
      ],
      "metadata": {
        "id": "0O4-6aFoOJVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR / \"tokenizer_final\")\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "vbwlS9EDOIY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Eval Loop"
      ],
      "metadata": {
        "id": "uxu_fx-5ON1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation_preview(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    dataset: Dataset,\n",
        "    num_samples: int = 3,\n",
        "    max_new_tokens: int = 128,\n",
        "    temperature: float = 0.3,\n",
        "    top_p: float = 0.9,\n",
        "    do_sample: bool = True,\n",
        "    use_kv_cache: bool = True,\n",
        "    history_mode: str = \"reference\",\n",
        "    inject_reference_responses: bool = False,\n",
        "    seed: int = 42,\n",
        "    output_dir: Path = EVAL_LOG_DIR,\n",
        "    file_prefix: str = \"baseline2_eval_loop\",\n",
        ") -> Path:\n",
        "    if history_mode not in {\"reference\", \"model\"}:\n",
        "        raise ValueError(\"history_mode must be 'reference' or 'model'\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_path = output_dir / f\"{file_prefix}_{timestamp}.txt\"\n",
        "    rng = random.Random(seed)\n",
        "    indices = list(range(len(dataset)))\n",
        "    if not indices:\n",
        "        raise ValueError(\"Dataset is empty.\")\n",
        "    rng.shuffle(indices)\n",
        "    selected_indices = indices[: min(num_samples, len(indices))]\n",
        "\n",
        "    lines: List[str] = []\n",
        "    model.eval()\n",
        "    original_use_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = use_kv_cache\n",
        "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for sample_rank, dataset_idx in enumerate(tqdm(selected_indices, desc=\"validation preview\"), start=1):\n",
        "            sample = dataset[dataset_idx]\n",
        "            normalized_messages = sample.get(\"normalized_messages\") or normalize_messages(sample.get(\"messages\", []))\n",
        "            history_reference: List[Dict[str, str]] = []\n",
        "            history_model: List[Dict[str, str]] = []\n",
        "            turn_outputs: List[Dict[str, str]] = []\n",
        "\n",
        "            for msg in normalized_messages:\n",
        "                role = msg[\"role\"]\n",
        "                content = msg[\"content\"]\n",
        "                if not content:\n",
        "                    continue\n",
        "                if role == \"system\":\n",
        "                    history_reference.append({\"role\": role, \"content\": content})\n",
        "                    history_model.append({\"role\": role, \"content\": content})\n",
        "                    continue\n",
        "\n",
        "                if role == \"user\":\n",
        "                    history_reference.append({\"role\": role, \"content\": content})\n",
        "                    if history_mode == \"model\":\n",
        "                        history_model.append({\"role\": role, \"content\": content})\n",
        "                    turn_outputs.append({\n",
        "                        \"question\": content,\n",
        "                        \"model_response\": \"\",\n",
        "                        \"ground_truth\": \"\",\n",
        "                    })\n",
        "                    prompt_history = history_model if history_mode == \"model\" else history_reference\n",
        "                    prompt_inputs = tokenizer.apply_chat_template(\n",
        "                        prompt_history,\n",
        "                        add_generation_prompt=True,\n",
        "                        return_tensors=\"pt\",\n",
        "                    )\n",
        "                    if isinstance(prompt_inputs, torch.Tensor):\n",
        "                        prompt_tensor = prompt_inputs.to(model.device)\n",
        "                        prompt_inputs = {\"input_ids\": prompt_tensor}\n",
        "                    else:\n",
        "                        prompt_inputs = {k: v.to(model.device) for k, v in prompt_inputs.items()}\n",
        "                    if \"attention_mask\" not in prompt_inputs:\n",
        "                        attention_mask = torch.ones_like(prompt_inputs[\"input_ids\"], dtype=torch.long)\n",
        "                        prompt_inputs[\"attention_mask\"] = attention_mask\n",
        "                    generation_kwargs = dict(\n",
        "                        max_new_tokens=max_new_tokens,\n",
        "                        temperature=temperature,\n",
        "                        top_p=top_p,\n",
        "                        do_sample=do_sample,\n",
        "                        pad_token_id=pad_token_id,\n",
        "                        eos_token_id=eos_token_id,\n",
        "                        use_cache=use_kv_cache,\n",
        "                    )\n",
        "                    generated_ids = model.generate(**prompt_inputs, **generation_kwargs)\n",
        "                    input_length = prompt_inputs[\"input_ids\"].shape[-1]\n",
        "                    new_tokens = generated_ids[0][input_length:]\n",
        "                    decoded = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "                    if not decoded:\n",
        "                        decoded = \"<empty>\"\n",
        "                    turn_outputs[-1][\"model_response\"] = decoded\n",
        "                    if history_mode == \"model\":\n",
        "                        history_model.append({\"role\": \"assistant\", \"content\": decoded})\n",
        "\n",
        "                elif role == \"assistant\":\n",
        "                    history_reference.append({\"role\": role, \"content\": content})\n",
        "                    if history_mode == \"reference\":\n",
        "                        history_model.append({\"role\": role, \"content\": content})\n",
        "                    elif history_mode == \"model\" and inject_reference_responses:\n",
        "                        history_model.append({\"role\": role, \"content\": content})\n",
        "                    if turn_outputs:\n",
        "                        turn_outputs[-1][\"ground_truth\"] = content\n",
        "\n",
        "            lines.append(f\"sample{sample_rank}-\")\n",
        "            for turn_idx, turn in enumerate(turn_outputs, start=1):\n",
        "                lines.append(f\"질문{turn_idx}: user:{turn['question']}\")\n",
        "                lines.append(f\"답변(모델): {turn['model_response']}\")\n",
        "                ground_truth = turn[\"ground_truth\"] or \"<no ground truth>\"\n",
        "                lines.append(f\"답변(원본): {ground_truth}\")\n",
        "            lines.append(\"\")\n",
        "\n",
        "    model.config.use_cache = original_use_cache\n",
        "    output_content = \"\".join(lines).strip() + \"\"\n",
        "    output_path.write_text(output_content, encoding=\"utf-8\")\n",
        "    print(f\"저장 완료: {output_path}\")\n",
        "    print(output_content)\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "cyfuR0eyOO-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "EPDe791TOTta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel # Import PeftModel\n",
        "\n",
        "####자신의 경로 지정\n",
        "adapter_dir = OUTPUT_DIR / \"checkpoint-2444\"\n",
        "tokenizer_dir = OUTPUT_DIR / \"tokenizer_final\"\n",
        "\n",
        "if PeftModel is None:\n",
        "    raise ImportError(\"peft.PeftModel is unavailable. Install the peft package first.\")\n",
        "if not adapter_dir.exists():\n",
        "    raise FileNotFoundError(f\"LoRA adapter directory not found: {adapter_dir}\")\n",
        "if not Path(tokenizer_dir).exists():\n",
        "    print(f\"Warning: {tokenizer_dir} not found. Falling back to base tokenizer {BASE_MODEL_NAME}.\")\n",
        "    tokenizer_dir = BASE_MODEL_NAME\n",
        "\n",
        "finetuned_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    tokenizer_dir,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "if finetuned_tokenizer.pad_token is None:\n",
        "    finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token\n",
        "\n",
        "finetuned_quant_config = get_quantization_config(load_in_4bit=True)\n",
        "finetuned_model = load_base_model(\n",
        "    BASE_MODEL_NAME,\n",
        "    quantization_config=finetuned_quant_config,\n",
        "    for_training=False,\n",
        ")\n",
        "finetuned_model = PeftModel.from_pretrained(finetuned_model, adapter_dir, is_trainable=False)\n",
        "finetuned_model.eval()\n",
        "\n",
        "finetuned_preview_log = run_validation_preview(\n",
        "    model=finetuned_model,\n",
        "    tokenizer=finetuned_tokenizer,\n",
        "    dataset=processed_datasets[\"validation\"],\n",
        "    num_samples=3,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    use_kv_cache=True,\n",
        "    history_mode=\"reference\",\n",
        "    seed=SEED,\n",
        "    file_prefix=\"baseline4_finetuned_eval\",\n",
        ")\n",
        "print(f\"Fine-tuned model eval log: {finetuned_preview_log}\")"
      ],
      "metadata": {
        "id": "O98IqKeAOVTR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}