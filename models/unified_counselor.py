"""
Unified Counselor Node
- ì§ˆì˜ì‘ë‹µ(QA) ì²˜ë¦¬
- Few-shot ì˜ˆì‹œ í™œìš© (1-2ê°œ)
- ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ë‹µë³€

Input: GraphState
  - user_query: str
  - recent_context: List[Message] (ìµœê·¼ 8í„´)
  - retrieved_examples: List[Dict] (1-2ê°œ)

Process:
  - Few-shot ì˜ˆì‹œ í¬ë§·íŒ…
  - ëŒ€í™” ë§¥ë½ í¬í•¨
  - LLM í˜¸ì¶œ

Output: GraphState
  - model_response: str
  
"""

import os
from typing import TYPE_CHECKING, List, Dict
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

if TYPE_CHECKING:
    from graph import GraphState, Message


def unified_counselor(state: "GraphState") -> "GraphState":
    """
    ì§ˆì˜ì‘ë‹µ ëª¨ë¸ (ë‹¨ìˆœí™”)
    - Few-shot ì˜ˆì‹œ í™œìš©
    - ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ë‹µë³€
    """
    
    try:
        # LLM ì´ˆê¸°í™”
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        user_query = state["user_query"]
        recent_context = state.get("recent_context", [])
        retrieved_examples = state.get("retrieved_examples", [])
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_text = build_qa_prompt(
            user_query=user_query,
            recent_context=recent_context,
            retrieved_examples=retrieved_examples
        )
        
        # LLM í˜¸ì¶œ
        response = llm.invoke(prompt_text)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["model_response"] = response.content.strip()
        
        print(f"[Unified Counselor] ë‹µë³€ ìƒì„± ì™„ë£Œ ({len(response.content)}ì)")
        
    except Exception as e:
        print(f"[Unified Counselor] Error: {e}")
        state["model_response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        state["error"] = f"í†µí•© ëª¨ë¸ ì˜¤ë¥˜: {str(e)}"
    
    return state


def build_qa_prompt(
    user_query: str,
    recent_context: List["Message"],
    retrieved_examples: List[Dict]
) -> str:
    """
    QA í”„ë¡¬í”„íŠ¸ ìƒì„±
    - Few-shot ì˜ˆì‹œ í™œìš©
    - ìµœê·¼ ëŒ€í™” ë§¥ë½ í¬í•¨
    - ë©”íƒ€ë°ì´í„° ì •ë³´ í¬í•¨
    """
    
    # 1. Few-shot ì˜ˆì‹œ í¬ë§·íŒ… (ë©”íƒ€ë°ì´í„° í¬í•¨)
    examples_text = ""
    if retrieved_examples:
        examples_text = "\n# ğŸ“š ìœ ì‚¬í•œ ìƒë‹´ ì˜ˆì‹œ:\n"
        for i, ex in enumerate(retrieved_examples, 1):
            examples_text += f"\n[ì˜ˆì‹œ {i}]\n"
            
            # âœ¨ ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
            if ex.get('domain') or ex.get('task_category') or ex.get('source'):
                examples_text += "ë©”íƒ€ë°ì´í„°:\n"
                if ex.get('domain'):
                    examples_text += f"  - ë„ë©”ì¸: {ex['domain']}\n"
                if ex.get('task_category'):
                    examples_text += f"  - ì§ˆë¬¸ ìœ í˜•: {ex['task_category']}\n"
                if ex.get('source'):
                    examples_text += f"  - ì¶œì²˜: {ex['source']}\n"
                examples_text += "\n"
            
            # ìƒë‹´ ëŒ€í™” í¬í•¨
            if ex.get('conversation'):
                conv_preview = ex['conversation'][:300]  # 300ìë¡œ ì œí•œ
                examples_text += f"ìƒë‹´ ëŒ€í™”:\n{conv_preview}...\n\n"
            
            examples_text += f"ì§ˆë¬¸: {ex['instruction']}\n"
            examples_text += f"ë‹µë³€: {ex['output']}\n"
    
    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

## ë‹µë³€ ì›ì¹™
1. êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ë‹µë³€ ì œê³µ
2. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€
3. ìœ ì‚¬ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ë˜, í˜„ì¬ ìƒí™©ì— ë§ê²Œ ë‹µë³€
4. ì •ë³´ê°€ ë¶ˆí™•ì‹¤í•˜ë©´ "í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤" ì•ˆë‚´
5. ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬
6. ê°„ê²°í•˜ê²Œ ë‹µë³€ (ë¶ˆí•„ìš”í•œ ì„¤ëª… ìì œ)
"""
    
    # 3. ìœ ì‚¬ ì˜ˆì‹œ ì¶”ê°€
    if examples_text:
        system_prompt += examples_text
    
    # 4. ëŒ€í™” ë§¥ë½ í¬ë§·íŒ…
    context_text = "\n## ğŸ’¬ ì´ì „ ëŒ€í™” ë§¥ë½:\n"
    
    if recent_context:
        for msg in recent_context:
            role = "ê³ ê°" if msg["role"] == "user" else "ìƒë‹´ì‚¬"
            context_text += f"{role}: {msg['content']}\n"
    else:
        context_text += "(ì²« ëŒ€í™”ì…ë‹ˆë‹¤)\n"
    
    # 5. í˜„ì¬ ì§ˆë¬¸
    query_text = f"\n## â“ í˜„ì¬ ì§ˆë¬¸:\nê³ ê°: {user_query}\n\nìƒë‹´ì‚¬: "
    
    return system_prompt + context_text + query_text