"""
Unified Counselor Node (ë©€í‹° í™˜ê²½ ì§€ì›)
- Mac (Ollama) / GPU (vLLM) ìë™ ê°ì§€
- KV Cache / Prefix Cachingìœ¼ë¡œ ë©€í‹°í„´ ìµœì í™”
- Few-shot ì˜ˆì‹œ í™œìš©
- ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ë‹µë³€

Input: GraphState
  - user_query: str
  - recent_context: List[Message] (ìµœê·¼ 8í„´)
  - retrieved_examples: List[Dict] (1-2ê°œ)

Process:
  - í™˜ê²½ ìë™ ê°ì§€
  - Few-shot ì˜ˆì‹œ í¬ë§·íŒ…
  - ëŒ€í™” ë§¥ë½ í¬í•¨ (KV cache ìµœì í™”)
  - LLM í˜¸ì¶œ

Output: GraphState
  - model_response: str
"""

from typing import TYPE_CHECKING, List, Dict, Iterator
from .env_detector import detect_environment
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from graph import GraphState, Message


# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_model_wrapper = None


def reset_model_wrapper():
    """ëª¨ë¸ wrapper ì‹±ê¸€í†¤ ì´ˆê¸°í™” (ëª¨ë¸ ë³€ê²½ ì‹œ ì‚¬ìš©)"""
    global _model_wrapper
    _model_wrapper = None
    logger.info("[Unified Counselor] ëª¨ë¸ wrapper ì´ˆê¸°í™”ë¨")


def get_model_wrapper():
    """í™˜ê²½ì— ë§ëŠ” ëª¨ë¸ wrapper ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _model_wrapper

    if _model_wrapper is None:
        env = detect_environment()

        if env == "gpu":
            from .vllm_wrapper import get_vllm_wrapper
            _model_wrapper = get_vllm_wrapper()
            print("[Unified Counselor] vLLM ëª¨ë“œ (Prefix Caching í™œì„±í™”)")

        else:  # mac
            from .ollama_wrapper import get_ollama_wrapper
            _model_wrapper = get_ollama_wrapper()
            print("[Unified Counselor] Ollama ëª¨ë“œ (KV Cache ìë™ ê´€ë¦¬)")

    return _model_wrapper


def unified_counselor(state: "GraphState") -> "GraphState":
    """
    ì§ˆì˜ì‘ë‹µ ëª¨ë¸ (ë©€í‹° í™˜ê²½ ì§€ì›)
    - Mac â†’ Ollama (KV cache)
    - GPU â†’ vLLM (Prefix caching)
    - Few-shot ì˜ˆì‹œ í™œìš©
    - ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ë‹µë³€
    """

    try:
        # ëª¨ë¸ wrapper ê°€ì ¸ì˜¤ê¸°
        model = get_model_wrapper()

        user_query = state["user_query"]
        recent_context = state.get("recent_context", [])
        retrieved_examples = state.get("retrieved_examples", [])

        # í”„ë¡¬í”„íŠ¸ ìƒì„± (KV cache ìµœì í™”ë¥¼ ìœ„í•´ êµ¬ì¡°í™”)
        prompt_text = build_qa_prompt(
            user_query=user_query,
            recent_context=recent_context,
            retrieved_examples=retrieved_examples
        )

        # ëª¨ë¸ í˜¸ì¶œ (KV cache / Prefix caching ìë™ ì ìš©)
        response_text = model.generate(
            prompt=prompt_text,
            temperature=0.3,
            top_p=0.9,
            max_tokens=512,
            stop=["ê³ ê°:", "\n\n\n"]
        )

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["model_response"] = response_text

        print(f"[Unified Counselor] ë‹µë³€ ìƒì„± ì™„ë£Œ ({len(response_text)}ì)")

    except Exception as e:
        print(f"[Unified Counselor] Error: {e}")
        import traceback
        traceback.print_exc()
        state["model_response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        state["error"] = f"í†µí•© ëª¨ë¸ ì˜¤ë¥˜: {str(e)}"

    return state


def build_qa_prompt(
    user_query: str,
    recent_context: List["Message"],
    retrieved_examples: List[Dict]
) -> str:
    """
    QA í”„ë¡¬í”„íŠ¸ ìƒì„± (KV Cache ìµœì í™” êµ¬ì¡°)

    êµ¬ì¡°:
    1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê³ ì •) â† KV cacheì—ì„œ ì¬ì‚¬ìš©
    2. Few-shot ì˜ˆì‹œ (ê³ ì •) â† KV cacheì—ì„œ ì¬ì‚¬ìš©
    3. ëŒ€í™” ë§¥ë½ (ë³€ë™) â† ìƒˆë¡œ ê³„ì‚°
    4. í˜„ì¬ ì§ˆë¬¸ (ë³€ë™) â† ìƒˆë¡œ ê³„ì‚°
    """

    # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê³ ì • - KV cache ì¬ì‚¬ìš©ë¨)
    system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

## ë‹µë³€ ì›ì¹™
1. êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ë‹µë³€ ì œê³µ
2. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€
3. ìœ ì‚¬ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ë˜, í˜„ì¬ ìƒí™©ì— ë§ê²Œ ë‹µë³€
4. ì •ë³´ê°€ ë¶ˆí™•ì‹¤í•˜ë©´ "í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤" ì•ˆë‚´
5. ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬
6. ê°„ê²°í•˜ê²Œ ë‹µë³€ (ë¶ˆí•„ìš”í•œ ì„¤ëª… ìì œ)
"""

    # 2. Few-shot ì˜ˆì‹œ í¬ë§·íŒ… (ê³ ì • - KV cache ì¬ì‚¬ìš©ë¨)
    examples_text = ""
    if retrieved_examples:
        examples_text = "\n# ğŸ“š ìœ ì‚¬í•œ ìƒë‹´ ì˜ˆì‹œ:\n"
        for i, ex in enumerate(retrieved_examples, 1):
            examples_text += f"\n[ì˜ˆì‹œ {i}]\n"

            # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
            if ex.get('domain') or ex.get('task_category') or ex.get('source'):
                examples_text += "ë©”íƒ€ë°ì´í„°:\n"
                if ex.get('domain'):
                    examples_text += f"  - ë„ë©”ì¸: {ex['domain']}\n"
                if ex.get('task_category'):
                    examples_text += f"  - ì§ˆë¬¸ ìœ í˜•: {ex['task_category']}\n"
                if ex.get('source'):
                    examples_text += f"  - ì¶œì²˜: {ex['source']}\n"
                examples_text += "\n"

            # ìƒë‹´ ëŒ€í™” í¬í•¨
            if ex.get('conversation'):
                conv_preview = ex['conversation'][:300]
                examples_text += f"ìƒë‹´ ëŒ€í™”:\n{conv_preview}...\n\n"

            examples_text += f"ì§ˆë¬¸: {ex['instruction']}\n"
            examples_text += f"ë‹µë³€: {ex['output']}\n"

    # 3. ëŒ€í™” ë§¥ë½ í¬ë§·íŒ… (ë³€ë™ - ë§¤ë²ˆ ìƒˆë¡œ ê³„ì‚°)
    context_text = "\n## ğŸ’¬ ì´ì „ ëŒ€í™” ë§¥ë½:\n"

    if recent_context:
        for msg in recent_context:
            role = "ê³ ê°" if msg["role"] == "user" else "ìƒë‹´ì‚¬"
            context_text += f"{role}: {msg['content']}\n"
    else:
        context_text += "(ì²« ëŒ€í™”ì…ë‹ˆë‹¤)\n"

    # 4. í˜„ì¬ ì§ˆë¬¸ (ë³€ë™ - ë§¤ë²ˆ ìƒˆë¡œ ê³„ì‚°)
    query_text = f"\n## â“ í˜„ì¬ ì§ˆë¬¸:\nê³ ê°: {user_query}\n\nìƒë‹´ì‚¬: "

    return system_prompt + examples_text + context_text + query_text


def unified_counselor_stream(
    user_query: str,
    recent_context: List["Message"],
    retrieved_examples: List[Dict]
) -> Iterator[str]:
    """
    ì§ˆì˜ì‘ë‹µ ëª¨ë¸ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
    - Mac â†’ Ollama (KV cache)
    - GPU â†’ vLLM (Prefix caching)
    - Few-shot ì˜ˆì‹œ í™œìš©
    - ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ë‹µë³€

    Yields:
        ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬
    """

    try:
        # ëª¨ë¸ wrapper ê°€ì ¸ì˜¤ê¸°
        model = get_model_wrapper()

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_text = build_qa_prompt(
            user_query=user_query,
            recent_context=recent_context,
            retrieved_examples=retrieved_examples
        )

        logger.info(f"[Unified Counselor Stream] ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì‹œì‘")

        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸
        if hasattr(model, 'generate_stream'):
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ìƒì„±
            for chunk in model.generate_stream(
                prompt=prompt_text,
                temperature=0.3,
                top_p=0.9,
                max_tokens=512,
                stop=["ê³ ê°:", "\n\n\n"]
            ):
                yield chunk
        else:
            # ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì› ì‹œ ì¼ë°˜ ìƒì„± í›„ í•œ ë²ˆì— ë°˜í™˜
            logger.warning(f"[Unified Counselor Stream] ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›, ì¼ë°˜ ìƒì„± ëª¨ë“œ ì‚¬ìš©")
            response = model.generate(
                prompt=prompt_text,
                temperature=0.3,
                top_p=0.9,
                max_tokens=512,
                stop=["ê³ ê°:", "\n\n\n"]
            )
            yield response

        logger.info(f"[Unified Counselor Stream] ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì™„ë£Œ")

    except Exception as e:
        logger.error(f"[Unified Counselor Stream] Error: {e}")
        import traceback
        traceback.print_exc()
        yield "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
