"""
LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©

Usage:
    python merge_lora_adapter.py
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

# ê²½ë¡œ ì„¤ì •
ADAPTER_PATH = "./Finetuning_Models/llama_3.1"
BASE_MODEL = "meta-llama/Llama-3.1-8B-Instruct"
OUTPUT_PATH = "./Finetuning_Models/llama_3.1_merged"

def merge_adapter():
    """LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©"""

    print("=" * 60)
    print("LoRA ì–´ëŒ‘í„° ë³‘í•© ì‹œì‘")
    print("=" * 60)

    # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    print(f"\n[1/4] ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘: {BASE_MODEL}")
    print("âš ï¸  Llama 3.1 8B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. Hugging Face í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   huggingface-cli login")
        print("2. meta-llama ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤:")
        print("   https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct")
        return False

    # 2. LoRA ì–´ëŒ‘í„° ë¡œë“œ
    print(f"\n[2/4] LoRA ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {ADAPTER_PATH}")
    try:
        model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
        print("âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    # 3. ë³‘í•©
    print(f"\n[3/4] LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•© ì¤‘...")
    try:
        model = model.merge_and_unload()
        print("âœ… ë³‘í•© ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë³‘í•© ì‹¤íŒ¨: {e}")
        return False

    # 4. ì €ì¥
    print(f"\n[4/4] ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘: {OUTPUT_PATH}")
    try:
        os.makedirs(OUTPUT_PATH, exist_ok=True)

        # ëª¨ë¸ ì €ì¥
        model.save_pretrained(
            OUTPUT_PATH,
            safe_serialization=True,
            max_shard_size="2GB"
        )

        # í† í¬ë‚˜ì´ì € ì €ì¥
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
        tokenizer.save_pretrained(OUTPUT_PATH)

        print("âœ… ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        print(f"   ì €ì¥ ìœ„ì¹˜: {OUTPUT_PATH}")

        # ì €ì¥ëœ íŒŒì¼ í™•ì¸
        print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
        for file in os.listdir(OUTPUT_PATH):
            file_path = os.path.join(OUTPUT_PATH, file)
            if os.path.isfile(file_path):
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                print(f"   - {file} ({size_mb:.1f} MB)")

    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 60)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. Ollama Modelfile ìƒì„±:")
    print("   python create_ollama_modelfile.py")
    print("2. Ollamaì— ëª¨ë¸ ë“±ë¡:")
    print("   ollama create llama3.1-counselor -f Modelfile")

    return True


if __name__ == "__main__":
    # ì˜ì¡´ì„± ì²´í¬
    try:
        import transformers
        import peft
        print(f"âœ… transformers ë²„ì „: {transformers.__version__}")
        print(f"âœ… peft ë²„ì „: {peft.__version__}")
    except ImportError as e:
        print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
        print("\nì„¤ì¹˜ ëª…ë ¹ì–´:")
        print("  pip install transformers peft accelerate bitsandbytes")
        exit(1)

    # GPU í™•ì¸
    if torch.cuda.is_available():
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        print("âœ… Apple Silicon (MPS) ì‚¬ìš© ê°€ëŠ¥")
    else:
        print("âš ï¸  CPU ëª¨ë“œ (ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

    print()

    # ë³‘í•© ì‹¤í–‰
    success = merge_adapter()

    if not success:
        exit(1)
