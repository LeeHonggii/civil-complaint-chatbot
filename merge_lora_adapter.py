"""
LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©

Usage:
    python merge_lora_adapter.py --model llama    # Llama 3.1 8B (ê¸°ë³¸ê°’)
    python merge_lora_adapter.py --model mistral  # Mistral 7B v0.2
    python merge_lora_adapter.py --model gemma    # Gemma 2 9B
    python merge_lora_adapter.py --model bccard   # BCCard Llama 3 8B
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os
import argparse

# ëª¨ë¸ë³„ ì„¤ì •
MODEL_CONFIGS = {
    "llama": {
        "adapter_path": "./Finetuning_Models/llama_3.1",
        "base_model": "meta-llama/Llama-3.1-8B-Instruct",
        "output_path": "./Finetuning_Models/llama_3.1_merged",
        "ollama_name": "llama3.1-counselor",
        "display_name": "Llama 3.1 8B Instruct"
    },
    "mistral": {
        "adapter_path": "./Finetuning_Models/mistral-7b-instruct-v0.2",
        "base_model": "mistralai/Mistral-7B-Instruct-v0.2",
        "output_path": "./Finetuning_Models/mistral_7b_merged",
        "ollama_name": "mistral7b-counselor",
        "display_name": "Mistral 7B Instruct v0.2"
    },
    "gemma": {
        "adapter_path": "./Finetuning_Models/gemma-2-9b-it-sft-lora",
        "base_model": "google/gemma-2-9b-it",
        "output_path": "./Finetuning_Models/gemma_2_9b_merged",
        "ollama_name": "gemma2-counselor",
        "display_name": "Gemma 2 9B Instruct"
    },
    "bccard": {
        "adapter_path": "./Finetuning_Models/llama-3-kor-bccard-finance",
        "base_model": "BCCard/Llama-3-Kor-BCCard-Finance-8B",
        "output_path": "./Finetuning_Models/bccard_llama3_merged",
        "ollama_name": "bccard-llama3-counselor",
        "display_name": "BCCard Llama 3 8B"
    }
}

def merge_adapter(config):
    """LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©"""

    adapter_path = config["adapter_path"]
    base_model = config["base_model"]
    output_path = config["output_path"]
    display_name = config["display_name"]
    ollama_name = config["ollama_name"]

    print("=" * 60)
    print(f"LoRA ì–´ëŒ‘í„° ë³‘í•© ì‹œì‘: {display_name}")
    print("=" * 60)

    # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    print(f"\n[1/4] ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘: {base_model}")
    print(f"âš ï¸  {display_name} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

    try:
        base_model_obj = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. Hugging Face í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("   huggingface-cli login")
        print(f"2. {display_name} ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤:")
        print(f"   https://huggingface.co/{base_model}")
        return False

    # 2. LoRA ì–´ëŒ‘í„° ë¡œë“œ
    print(f"\n[2/4] LoRA ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {adapter_path}")
    try:
        model = PeftModel.from_pretrained(
            base_model_obj,
            adapter_path,
            is_trainable=False
        )
        print("âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    # 3. ë³‘í•©
    print(f"\n[3/4] LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•© ì¤‘...")
    try:
        model = model.merge_and_unload()
        print("âœ… ë³‘í•© ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë³‘í•© ì‹¤íŒ¨: {e}")
        return False

    # 4. ì €ì¥
    print(f"\n[4/4] ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘: {output_path}")
    try:
        os.makedirs(output_path, exist_ok=True)

        # ëª¨ë¸ ì €ì¥
        model.save_pretrained(
            output_path,
            safe_serialization=True,
            max_shard_size="2GB"
        )

        # í† í¬ë‚˜ì´ì € ì €ì¥
        tokenizer = AutoTokenizer.from_pretrained(base_model)
        tokenizer.save_pretrained(output_path)

        print("âœ… ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        print(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")

        # ì €ì¥ëœ íŒŒì¼ í™•ì¸
        print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
        for file in os.listdir(output_path):
            file_path = os.path.join(output_path, file)
            if os.path.isfile(file_path):
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                print(f"   - {file} ({size_mb:.1f} MB)")

    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 60)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print(f"1. Ollamaì— ëª¨ë¸ ë“±ë¡:")
    print(f"   ollama create {ollama_name} -f Modelfile.{ollama_name.replace('-counselor', '')}")
    print(f"2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env):")
    print(f"   OLLAMA_MODEL_NAME={ollama_name}")
    print(f"   FINETUNED_MODEL_PATH={output_path}")

    return True


if __name__ == "__main__":
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©")
    parser.add_argument(
        "--model",
        type=str,
        choices=["llama", "mistral", "gemma", "bccard"],
        default="llama",
        help="ë³‘í•©í•  ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: llama)"
    )
    args = parser.parse_args()

    # ì„ íƒëœ ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    config = MODEL_CONFIGS[args.model]

    print("\n" + "=" * 60)
    print(f"ì„ íƒëœ ëª¨ë¸: {config['display_name']}")
    print("=" * 60)

    # ì˜ì¡´ì„± ì²´í¬
    try:
        import transformers
        import peft
        print(f"\nâœ… transformers ë²„ì „: {transformers.__version__}")
        print(f"âœ… peft ë²„ì „: {peft.__version__}")
    except ImportError as e:
        print(f"\nâŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
        print("\nì„¤ì¹˜ ëª…ë ¹ì–´:")
        print("  pip install transformers peft accelerate bitsandbytes")
        exit(1)

    # GPU í™•ì¸
    if torch.cuda.is_available():
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        print("âœ… Apple Silicon (MPS) ì‚¬ìš© ê°€ëŠ¥")
    else:
        print("âš ï¸  CPU ëª¨ë“œ (ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

    print()

    # ë³‘í•© ì‹¤í–‰
    success = merge_adapter(config)

    if not success:
        exit(1)
