"""
ìƒë‹´ ì±—ë´‡ - Streamlit UI
- ë©€í‹°í„´ ëŒ€í™” ì§€ì›
- ì„¸ì…˜ ê´€ë¦¬
- LangGraph íŒŒì´í”„ë¼ì¸ í†µí•©
"""

import streamlit as st
import os
from dotenv import load_dotenv
from graph import run_graph, session_store, format_conversation_history
from models import initialize_vector_store

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìƒë‹´ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="centered"
)

# íƒ€ì´í‹€
st.title("ğŸ¤– ìƒë‹´ ì±—ë´‡")
st.caption("ê³ ê° ìƒë‹´ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤")

# Vector Store ì´ˆê¸°í™” (ìºì‹±)
@st.cache_resource
def get_vector_store():
    """Vector Store ì´ˆê¸°í™” (Streamlit ìºì‹±)"""
    initialize_vector_store()
    return True

# Vector Store ìë™ ì´ˆê¸°í™”
with st.spinner("ğŸ”§ Vector Store ì´ˆê¸°í™” ì¤‘..."):
    try:
        get_vector_store()
    except Exception as e:
        st.error(f"âŒ Vector Store ì´ˆê¸°í™” ì‹¤íŒ¨: ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # ëª¨ë¸ ì„ íƒ
    st.subheader("ğŸ¤– ëª¨ë¸ ì„ íƒ")

    # ëª¨ë¸ ì´ë¦„ ë§¤í•‘
    model_display_names = {
        "llama": "Llama 3.1 8B Instruct",
        "mistral": "Mistral 7B Instruct v0.2",
        "gemma": "Gemma 2 9B Instruct",
        "bccard": "BCCard Llama 3 8B"
    }

    # í˜„ì¬ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    model_choice = st.selectbox(
        "ì¶”ë¡  ëª¨ë¸",
        options=["llama", "mistral", "gemma", "bccard"],
        format_func=lambda x: model_display_names.get(x, x),
        index=["llama", "mistral", "gemma", "bccard"].index(os.getenv("MODEL_NAME", "llama")),
        help="ëŒ€í™” ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
    )

    # íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš© ì²´í¬ë°•ìŠ¤
    use_finetuned = st.checkbox(
        "íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©",
        value=os.getenv("USE_FINETUNED_MODEL", "false").lower() == "true",
        help="ì²´í¬í•˜ë©´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , ì²´í¬ í•´ì œí•˜ë©´ ë² ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤"
    )

    # ëª¨ë¸ ë³€ê²½ ì‹œ í™˜ê²½ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
    if "current_model" not in st.session_state:
        st.session_state.current_model = os.getenv("MODEL_NAME", "llama")

    if "use_finetuned" not in st.session_state:
        st.session_state.use_finetuned = os.getenv("USE_FINETUNED_MODEL", "false").lower() == "true"

    # ëª¨ë¸ ë˜ëŠ” íŒŒì¸íŠœë‹ ì„¤ì • ë³€ê²½ ê°ì§€
    model_changed = model_choice != st.session_state.current_model
    finetuned_changed = use_finetuned != st.session_state.use_finetuned

    if model_changed or finetuned_changed:
        os.environ["MODEL_NAME"] = model_choice
        os.environ["USE_FINETUNED_MODEL"] = "true" if use_finetuned else "false"
        st.session_state.current_model = model_choice
        st.session_state.use_finetuned = use_finetuned

        # ì‹±ê¸€í†¤ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        from models.unified_counselor import reset_model_wrapper
        from models.ollama_wrapper import reset_ollama_wrapper
        try:
            from models.vllm_wrapper import reset_vllm_wrapper
            reset_vllm_wrapper()
        except:
            pass  # GPU í™˜ê²½ì´ ì•„ë‹ˆë©´ vllm ëª¨ë“ˆì´ ì—†ì„ ìˆ˜ ìˆìŒ
        reset_model_wrapper()
        reset_ollama_wrapper()

        model_type = "íŒŒì¸íŠœë‹" if use_finetuned else "ë² ì´ìŠ¤"
        display_name = model_display_names.get(model_choice, model_choice)
        st.info(f"âœ¨ {display_name} {model_type} ëª¨ë¸ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ëŒ€í™”ë¶€í„° ì ìš©ë©ë‹ˆë‹¤.")

    # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ í‘œì‹œ
    model_type = "íŒŒì¸íŠœë‹" if use_finetuned else "ë² ì´ìŠ¤"
    display_name = model_display_names.get(model_choice, model_choice)
    st.caption(f"ğŸ¯ í˜„ì¬ ëª¨ë¸: {display_name} ({model_type})")

    st.divider()

    # ì´ˆê¸°í™” ìƒíƒœ í‘œì‹œ
    st.success("âœ… Vector Store ì¤€ë¹„ë¨")

    st.divider()

    # ìƒˆ ëŒ€í™” ì‹œì‘ ë²„íŠ¼
    if st.button("ğŸ”„ ìƒˆ ëŒ€í™” ì‹œì‘", use_container_width=True):
        if "session_id" in st.session_state:
            # ê¸°ì¡´ ì„¸ì…˜ í´ë¦¬ì–´
            session_store.clear_session(st.session_state.session_id)
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.messages = []
        st.session_state.session_id = None
        st.rerun()

    st.divider()

    # ì„¸ì…˜ ì •ë³´
    if "session_id" in st.session_state and st.session_state.session_id:
        st.caption(f"ğŸ“ ì„¸ì…˜ ID: {st.session_state.session_id[:8]}...")

        # ëŒ€í™” í„´ ìˆ˜
        history = session_store.get_conversation_history(st.session_state.session_id)
        st.caption(f"ğŸ’¬ ëŒ€í™” í„´: {len(history)//2}í„´")

# ë©”ì¸ í™”ë©´
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "session_id" not in st.session_state:
    st.session_state.session_id = None

# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        response = ""  # ì‘ë‹µ ë³€ìˆ˜ ì´ˆê¸°í™”
        response_placeholder = st.empty()

        try:
            # ë¡œë”© ë©”ì‹œì§€ í‘œì‹œ
            response_placeholder.markdown("ğŸ¤– ìƒì„±ì¤‘...")

            # LangGraph ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)
            result = run_graph(
                user_query=prompt,
                session_id=st.session_state.session_id,
                window_size=8,
                stream=True
            )

            # ì„¸ì…˜ ID ì €ì¥ (ì²« ì‘ë‹µì‹œ)
            if st.session_state.session_id is None:
                st.session_state.session_id = result["session_id"]

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í‘œì‹œ
            full_response = ""

            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸°
            if "response_stream" in result and result["response_stream"]:
                for chunk in result["response_stream"]:
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")
                response_placeholder.markdown(full_response)
                response = full_response

                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì €ì¥
                from graph import session_store
                session_store.add_message(result["session_id"], "user", prompt)
                session_store.add_message(result["session_id"], "assistant", response)
            else:
                # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì‘ë‹µ ì‚¬ìš©
                response = result.get("response", "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                response_placeholder.markdown(response)

                # ì—ëŸ¬ í‘œì‹œ (ìˆì„ ê²½ìš°)
                if result.get("error"):
                    st.error(f"âš ï¸ {result['error']}")

        except Exception as e:
            response = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            response_placeholder.markdown("")
            st.error(response)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": response})

# í•˜ë‹¨ ì •ë³´
st.divider()

# í‘¸í„°
model_name = os.getenv("MODEL_NAME", "llama")
use_finetuned = os.getenv("USE_FINETUNED_MODEL", "false").lower() == "true"
model_display_names = {
    "llama": "Llama 3.1 8B",
    "mistral": "Mistral 7B",
    "gemma": "Gemma 2 9B",
    "bccard": "BCCard Llama 3 8B"
}
model_display = model_display_names.get(model_name, "Unknown")
model_type = " (íŒŒì¸íŠœë‹)" if use_finetuned else " (ë² ì´ìŠ¤)"
st.caption(f"Powered by LangGraph + {model_display}{model_type} + Pinecone/ChromaDB")