"""
Ollama Modelfile ìƒì„±

ë³‘í•©ëœ ëª¨ë¸ì„ Ollamaë¡œ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ Modelfile ìƒì„±
"""

import os

# ê²½ë¡œ ì„¤ì •
MERGED_MODEL_PATH = "./Finetuning_Models/llama_3.1_merged"
MODELFILE_PATH = "./Modelfile"

def create_modelfile():
    """Ollama Modelfile ìƒì„±"""

    print("=" * 60)
    print("Ollama Modelfile ìƒì„±")
    print("=" * 60)

    # ë³‘í•©ëœ ëª¨ë¸ ì¡´ì¬ í™•ì¸
    if not os.path.exists(MERGED_MODEL_PATH):
        print(f"âŒ ë³‘í•©ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MERGED_MODEL_PATH}")
        print("\në¨¼ì € LoRA ì–´ëŒ‘í„°ë¥¼ ë³‘í•©í•˜ì„¸ìš”:")
        print("  python merge_lora_adapter.py")
        return False

    # Modelfile ë‚´ìš©
    modelfile_content = f"""# Llama 3.1 8B Counselor (Fine-tuned)
# ìƒë‹´ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸

FROM {os.path.abspath(MERGED_MODEL_PATH)}

# íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMETER temperature 0.3
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_ctx 4096
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|end_of_text|>"

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìƒë‹´ ì „ë¬¸ê°€)
SYSTEM \"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

## ë‹µë³€ ì›ì¹™
1. êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ë‹µë³€ ì œê³µ
2. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€
3. ì •ë³´ê°€ ë¶ˆí™•ì‹¤í•˜ë©´ "í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤" ì•ˆë‚´
4. ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬
5. ê°„ê²°í•˜ê²Œ ë‹µë³€ (ë¶ˆí•„ìš”í•œ ì„¤ëª… ìì œ)
\"\"\"

# í…œí”Œë¦¿ ì„¤ì • (Llama 3.1 í˜•ì‹)
TEMPLATE \"\"\"{{- if .System }}
<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>
{{- end }}
<|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

\"\"\"
"""

    # Modelfile ì €ì¥
    try:
        with open(MODELFILE_PATH, "w", encoding="utf-8") as f:
            f.write(modelfile_content)

        print(f"âœ… Modelfile ìƒì„± ì™„ë£Œ: {MODELFILE_PATH}")
        print("\nğŸ“„ ë‚´ìš©:")
        print("-" * 60)
        print(modelfile_content)
        print("-" * 60)

    except Exception as e:
        print(f"âŒ Modelfile ìƒì„± ì‹¤íŒ¨: {e}")
        return False

    print("\n" + "=" * 60)
    print("âœ… Modelfile ìƒì„± ì™„ë£Œ!")
    print("=" * 60)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. Ollamaì— ëª¨ë¸ ë“±ë¡:")
    print("   ollama create llama3.1-counselor -f Modelfile")
    print("\n2. ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
    print("   ollama run llama3.1-counselor \"ì•ˆë…•í•˜ì„¸ìš”\"")
    print("\n3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env):")
    print("   OLLAMA_MODEL_NAME=llama3.1-counselor")

    return True


if __name__ == "__main__":
    success = create_modelfile()

    if not success:
        exit(1)
